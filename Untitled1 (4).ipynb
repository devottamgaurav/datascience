{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "827e3eea-a123-4e98-ae47-895bbd2a7501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.21.6\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas) (2022.4)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.3.5\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.8/24.8 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.6 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.21.6)\n",
      "Collecting scipy>=1.1.0\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=0.11\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1304 sha256=ec9e3a8a3a90b7e53b675571148e939ecc7c353d459ad66ecb4294bee3940765\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.0.2 scipy-1.7.3 sklearn-0.0 threadpoolctl-3.1.0\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib) (1.21.6)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib) (3.0.9)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions in /srv/conda/envs/notebook/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (4.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pillow, kiwisolver, fonttools, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.38.0 kiwisolver-1.4.4 matplotlib-3.5.3 pillow-9.2.0\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.12.1-py3-none-any.whl (288 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from seaborn) (1.21.6)\n",
      "Requirement already satisfied: typing_extensions in /srv/conda/envs/notebook/lib/python3.7/site-packages (from seaborn) (4.4.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from seaborn) (3.5.3)\n",
      "Requirement already satisfied: pandas>=0.25 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.38.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas>=0.25->seaborn) (2022.4)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.12.1\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: graphviz\n",
      "Successfully installed graphviz-0.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "!pip install tabulate\n",
    "!pip install seaborn\n",
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca237b95-0d20-4a8a-b55e-7f84fb18231d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder as SklearnOneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, classification_report, \\\n",
    "    accuracy_score, f1_score\n",
    "from datetime import timedelta, date\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, classification_report, \\\n",
    "    accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import export_graphviz \n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import tree\n",
    "import zipfile\n",
    "import seaborn as sns\n",
    "import os\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a08df706-65ab-4f82-8bfe-14d2074f6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Assining_Variables():\n",
    "    fle1 = \"6M-0K-99K.users.dataset.public.csv\"\n",
    "    fle2 = \"Buyers-repartition-by-country.csv\"\n",
    "    fle3 = \"Comparison-of-Sellers-by-Gender-and-Country.csv\"\n",
    "    fle4 = \"Countries-with-Top-Sellers-(Fashion-C2C).csv\"\n",
    "    return fle1,fle2,fle3,fle4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58914523-583f-40fa-af58-2ae139ac4216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(fle1,fle2,fle3,fle4):\n",
    "    df_air_visit_data = pd.read_csv(fle1)\n",
    "    df_air_reserve = pd.read_csv(fle2)\n",
    "    df_air_store_info = pd.read_csv(fle3)\n",
    "    df_date_info = pd.read_csv(fle4)  \n",
    "    return df_air_visit_data,df_air_reserve,df_air_store_info,df_date_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e127c853-7fed-43c0-94c3-091c434e3e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    " def Counting_NAN_Values(dfff):\n",
    "        df_nan = pd.DataFrame(columns=['Nan count'])\n",
    "        nan_col = list(dfff.columns.tolist())\n",
    "        total_nan = 0\n",
    "        for col in nan_col:\n",
    "            nan_cnt = dfff[col].isnull().sum()\n",
    "            df_nan.loc[col] = nan_cnt\n",
    "            total_nan += nan_cnt\n",
    "        print(tabulate(df_nan, headers = 'keys', tablefmt = 'psql'))\n",
    "        return df_nan['Nan count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58cd7f99-3c28-463a-9969-b63781619fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dataset_Cleaning(dff):\n",
    "        dfff = dff.isna().sum().sum()\n",
    "        print(\"Total NAN values are : {0}\".format(dfff))\n",
    "        dff_nan = dff[pd.isnull(dff).any(axis=1)]\n",
    "        print(\"Data with at least 1 NAN values: {0}\".format(len(dff_nan.index)))\n",
    "        df_nan_all = dff[pd.isnull(dff).all(1)]\n",
    "        print(\"Rows filled NAN data: {0}\".format(len(df_nan_all.index)))\n",
    "        df_air_store = dff.identifierhash.isnull().sum()\n",
    "        #df_hpg_store = dff.hpg_store_id.isnull().sum()\n",
    "        print(\"Count of NAN values on identifierhash are: {0}\".format(df_air_store))\n",
    "        #print(\"Count of NAN values on hpg_store_id are: {0}\".format(df_hpg_store))\n",
    "        nan_ids_df = dff[dff.identifierhash.isnull()]\n",
    "        len_invalid_id = len(nan_ids_df.index)\n",
    "        print(\"Number of Invalid IDs (identifierhash) are : {0}\".format(len_invalid_id))\n",
    "        df_nadn = Counting_NAN_Values(dff)\n",
    "        return dfff,df_nadn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feb9df01-be12-4f3d-b234-222edaebec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pie_Chart_NAN_Column_Plotting(dft_val1,dfg,total_crt_value_cnt,nan_value_cnt):\n",
    "    dft = pd.DataFrame({'Data': ['Correct Values', 'NAN Values'],\n",
    "                          'ValueCount': [total_crt_value_cnt,nan_value_cnt]})\n",
    "        # Plotting the pie chart for above dataframe\n",
    "    #dft.groupby(['Data']).sum().plot(kind='pie', y='Value Count', autopct='%1.0f%%')\n",
    "    # Data to plot\n",
    "    lab1 = dft.Data.values\n",
    "    sdd = dft.ValueCount.values\n",
    "    #sizes = [215, 130,215, 130,215, 130]\n",
    "    color = ['lightcoral', 'lightskyblue']\n",
    "    explode1 = (0.1, 0)  # explode 1st slice\n",
    "    # Plot\n",
    "    plt.pie(sdd, explode=explode1, labels=lab1, colors=color,autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "   \n",
    "    # Define the ratio of gap of each fragment in a tuple\n",
    "    #explode = (0.05, 0.05, 0.05,0.05, 0.05, 0.05,0.05, 0.05, 0.05,0.05, 0.05, 0.05,0.05, 0.05)\n",
    "    # DataFrame of each student and the votes they get df_nan_disp['Data Name'],df_nan_disp['NAN Count for each Column']\n",
    "    #dft = pd.DataFrame({'Data Name': dfg.columns,'NAN Count for each Column':dft_val1})\n",
    "    # Plotting the pie chart for above dataframe\n",
    "    # Data to plot\n",
    "    #labels = dfg.columns.values\n",
    "    #sddd = dft_val1.values\n",
    "    #sizes = [215, 130,215, 130,215, 130,215, 130,215, 130,215, 130,215, 130]\n",
    "    #colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']\n",
    "    #explode = (0.1, 0,0.1, 0,0.1, 0,0.1, 0,0.1, 0,0.1, 0,0.1, 0)  # explode 1st slice\n",
    "    # Plot\n",
    "    #plt.pie(sddd, explode=explode, labels=labels, colors=colors,autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "    #plt.axis('equal')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd55cc85-6487-4780-9264-e2386ac83200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final_Dataset():\n",
    "        fle1,fle2,fle3,fle4 = Assining_Variables()\n",
    "        df_air_visit_data,df_air_reserve,df_air_store_info,df_date_info = load_dataset(fle1,fle2,fle3,fle4)\n",
    "        #df_air_visit_data,df_air_reserve,df_air_store_info,df_date_info,df_store_id_relation,df_hpg_reserve,df_hpg_store_info = self.load_dataset()\n",
    "        h1,t1 = os.path.split(fle1)\n",
    "        h2,t2 = os.path.split(fle2)\n",
    "        h3,t3 = os.path.split(fle3)\n",
    "        h4,t4 = os.path.split(fle4)\n",
    "        print(\"The shape of {0} is {1}\".format(t1,df_air_visit_data.shape),end=\"\\n\")\n",
    "        print(\"The shape of {0} is {1}\".format(t2,df_air_reserve.shape),end=\"\\n\")\n",
    "        print(\"The shape of {0} is {1}\".format(t3,df_air_store_info.shape),end=\"\\n\")\n",
    "        print(\"The shape of {0} is {1}\".format(t4,df_date_info.shape),end=\"\\n\")\n",
    "        \n",
    "        \"\"\"df_n = pd.concat([df_air_visit_data[['air_store_id','visit_date','visitors']],df_air_reserve], axis=0, join=\"outer\")\n",
    "        df_nn = pd.merge(df_n,df_air_store_info,on='air_store_id',how=\"left\")\n",
    "        df_nn = df_nn.rename(columns = {'air_genre_name':'genre_name','air_area_name':'area_name'})\n",
    "        hpg_n = pd.merge(df_hpg_store_info,df_hpg_reserve,on='hpg_store_id',how=\"left\")\n",
    "        #hpg_n = pd.concat([df_hpg_store_info,df_hpg_reserve,], axis=0, join=\"outer\")\n",
    "        hpg_n = hpg_n.rename(columns = {'hpg_genre_name':'genre_name','hpg_area_name':'area_name'})\n",
    "        hpg_nn = pd.merge(hpg_n,df_store_id_relation,on='hpg_store_id',how=\"left\")\n",
    "        hpg_date = pd.concat([hpg_nn,df_date_info,], axis=0, join=\"outer\")\n",
    "        df = pd.concat([df_nn,hpg_date], axis=0, join=\"outer\")\n",
    "        #hpg_date = [hpg_nn,df_date_info]\n",
    "        #df_date = pd.concat(hpg_date,axis=1)\n",
    "        #df_nnnn = [df_nn,df_date]\n",
    "        #df = pd.concat(df_nnnn,axis=0)\n",
    "        print(df)\n",
    "        \"\"\"\n",
    " \n",
    "        #Merging\n",
    "        \"\"\"df_n = pd.merge(df_air_visit_data[['air_store_id','visit_date','visitors']],df_air_reserve,how=\"outer\")      \n",
    "        df_nn = pd.merge(df_n,df_air_store_info,on='air_store_id',how=\"left\")\n",
    "        df_nn = df_nn.rename(columns = {'air_genre_name':'genre_name','air_area_name':'area_name'})\n",
    "        hpg_n = pd.merge(df_hpg_store_info,df_hpg_reserve,on='hpg_store_id',how=\"left\")\n",
    "        hpg_n = hpg_n.rename(columns = {'hpg_genre_name':'genre_name','hpg_area_name':'area_name'})\n",
    "        hpg_nn = pd.merge(hpg_n,df_store_id_relation,on='hpg_store_id',how=\"left\")\n",
    "        hpg_date = [hpg_nn,df_date_info]\n",
    "        df_date = pd.concat(hpg_date,axis=1)\n",
    "        df_nnnn = [df_nn,df_date]\n",
    "        df = pd.concat(df_nnnn,axis=0)\n",
    "        print(df)\"\"\"\n",
    "        #df_nadjn = Counting_NAN_Values(df)\n",
    "        \n",
    "        \"\"\"total_crt_value_cnt,nan_value_cnt,df_najn = Nan_Percent_Computation(df)\n",
    "        #Pie_Chart_Plotting(total_crt_value_cnt,nan_value_cnt)\n",
    "        df_nan_disp = pd.DataFrame()\n",
    "        Pie_Chart_NAN_Column_Plotting(df_najn,df,total_crt_value_cnt,nan_value_cnt)\"\"\"\n",
    "        \n",
    "        #Printing the number of NANs in each column\n",
    "        \"\"\"n_rows = len(df)\n",
    "        for i in range(0,len(df_najn)):\n",
    "            nan_percent_in_each_col = ((df_najn/n_rows)*100)\n",
    "            df_nan_disp = pd.DataFrame({'% of NAN Count':nan_percent_in_each_col})\n",
    "        print(tabulate(df_nan_disp, headers = 'keys', tablefmt = 'psql'))\"\"\"\n",
    "        #print(df)\n",
    "        #print(df)\n",
    "        \n",
    "        #df = df.dropna(subset=['visitors'],inplace=True)\n",
    "        #print(df)\n",
    "       # from sklearn.impute import SimpleImputer\n",
    "        #df = SimpleImputer(strategy='constant',fill_value='air_8093d0b565e9dbdf')\n",
    "        \"\"\"from sklearn.preprocessing import OneHotEncoder\n",
    "        from sklearn.compose import ColumnTransformer\"\"\"\n",
    "        \"\"\"cat_feat = ['air_store_id']\n",
    "        one_hot = OneHotEncoder()\n",
    "        transformer = ColumnTransformer([(\"one_hot\",one_hot,cat_feat)],remainder=\"passthrough\")\n",
    "        transformed_X = transformer.fit_transform(df)\n",
    "        print(transformed_X)\"\"\"\n",
    "        \"\"\"total_crt_value_cnt1,nan_value_cnt1,df_najn1 = Nan_Percent_Computation(df)\n",
    "        #Pie_Chart_Plotting(total_crt_value_cnt,nan_value_cnt)\n",
    "        df_nan_disp1 = pd.DataFrame()\n",
    "        #Pie_Chart_NAN_Column_Plotting(df_najn1,df,total_crt_value_cnt,nan_value_cnt)\n",
    "        \n",
    "        #Printing the number of NANs in each column\n",
    "        n_rows1 = len(df)\n",
    "        for i in range(0,len(df_najn1)):\n",
    "            nan_percent_in_each_col1 = ((df_najn1/n_rows1)*100)\n",
    "            df_nan_disp1 = pd.DataFrame({'% of NAN Count':nan_percent_in_each_col1})\n",
    "        print(tabulate(df_nan_disp1, headers = 'keys', tablefmt = 'psql'))\"\"\"\n",
    "        #print(df)\n",
    "        \"\"\"print(df.air_store_id.value_counts())\n",
    "        print(df.visit_date.value_counts())\n",
    "        print(df.visitors.value_counts())\n",
    "        print(df.visit_datetime.value_counts())\n",
    "        print(df.reserve_datetime.value_counts())\n",
    "        #print(df.reserve_visitors.value_counts())\n",
    "        print(df.genre_name.value_counts())\n",
    "        print(df.area_name.value_counts())\n",
    "        #print(df.latitude.value_counts())\n",
    "        #print(df.longitude.value_counts())\n",
    "        print(df.hpg_store_id.value_counts())\n",
    "        print(df.reserve_visitors.median())\"\"\"\n",
    "        \n",
    "        \"\"\"print(df.latitude.median())\n",
    "        print(df.longitude.median())\"\"\"\n",
    "        \n",
    "        \"\"\"df.drop(['calendar_date','day_of_week','holiday_flg'],axis=1,inplace=True)\n",
    "        #df.drop(df['calendar_date'],axis=1,inplace=True)\n",
    "        df[['visit_date','visitors','visit_datetime','reserve_datetime','reserve_visitors','hpg_store_id']] = df[['visit_date','visitors','visit_datetime','reserve_datetime','reserve_visitors','hpg_store_id']].replace(np.nan,9999)\n",
    "        #df[['latitude','longitude','visit_datetime','reserve_datetime','reserve_visitors','hpg_store_id','hpg_area_name','hpg_genre_name']] = df[['latitude','longitude','visit_datetime','reserve_datetime','reserve_visitors','hpg_store_id','hpg_area_name','hpg_genre_name']].replace(np.nan,9999)\n",
    "        df[['area_name','latitude','longitude','genre_name']] = df[['area_name','latitude','longitude','genre_name']].replace(np.nan,9999)\n",
    "        df.drop(df.index[df['latitude'] == 9999], inplace=True)\n",
    "        df.drop(df.index[df['longitude'] == 9999], inplace=True)\n",
    "        #df.drop(df.index[df['visit_datetime'] == 9999], inplace=True)\n",
    "        #df.drop(df.index[df['reserve_datetime'] == 9999], inplace=True)\n",
    "        #df.drop(df.index[df['reserve_visitors'] == 9999], inplace=True)\n",
    "        #df.drop(df.index[df['hpg_store_id'] == 9999], inplace=True)\"\"\"\n",
    "        #df['air_store_id'] = df['air_store_id'].replace(np.nan,9999)\n",
    "        #df.drop(df.index[df['air_store_id'] == 9999], inplace=True)\n",
    "        \"\"\"df.drop(df.index[df['area_name'] == 9999], inplace=True)\n",
    "        df.drop(df.index[df['genre_name'] == 9999], inplace=True)\n",
    "        #df[['visitors']] = df[['visitors']].astype(int)\n",
    "        #df.reset_index()\n",
    "        df['air_store_id'] = df['air_store_id'].replace(np.nan,\"air_8093d0b565e9dbdf\")\n",
    "        df.drop(df.index[df['air_store_id'] == \"air_8093d0b565e9dbdf\"], inplace=True)\n",
    "        df['visit_datetime'] = df['visit_datetime'].replace(np.nan,\"2016-12-22 19:00:00\")\n",
    "        #df.drop(df.index[df['visit_datetime'] == \"2016-12-22 19:00:00\"], inplace=True)\n",
    "        df['reserve_datetime'] = df['reserve_datetime'].replace(np.nan,\"2016-12-12 21:00:00\")\n",
    "        #df.drop(df.index[df['reserve_datetime'] == \"2016-12-12 21:00:00\"], inplace=True)\n",
    "        df['hpg_store_id'] = df['hpg_store_id'].replace(np.nan,\"hpg_011e799ba201ad2e\")\n",
    "        df['visit_date'] = df['visit_date'].replace(np.nan,\"17-03-2017\")\n",
    "        df['reserve_visitors'] = df['reserve_visitors'].replace(np.nan,3.0)\n",
    "        df['visitors'] = df['visitors'].replace(np.nan,8.0)\n",
    "        #df.drop(df.index[df['reserve_visitors'] == 2.0], inplace=True)\n",
    "        print(df)\n",
    "        df_nadbjn = Counting_NAN_Values(df)\"\"\"\n",
    "        #return df,fnl_flle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7247cee6-544b-4537-ac08-98fbc031df85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of 6M-0K-99K.users.dataset.public.csv is (98913, 24)\n",
      "The shape of Buyers-repartition-by-country.csv is (62, 32)\n",
      "The shape of Comparison-of-Sellers-by-Gender-and-Country.csv is (73, 19)\n",
      "The shape of Countries-with-Top-Sellers-(Fashion-C2C).csv is (19, 26)\n"
     ]
    }
   ],
   "source": [
    "Final_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c249aec-6643-443a-aaad-d3d7f1966e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
